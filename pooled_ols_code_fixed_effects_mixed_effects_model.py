# -*- coding: utf-8 -*-
"""Pooled OLS Code - Fixed Effects - Mixed Effects Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sSR6stZDJMtT2SUMnNfrq_thVEiV3ygO

# **Pooled OLS**
"""

import pandas as pd
import numpy as np
from statsmodels.api import OLS, add_constant

# Load the dataset
df = pd.read_excel("/path/to/your/dataset.xlsx", header=1, names=['Index', 'Farm ID', 'Date', 'NDVI', 'NDWI', 'CIG', 'Yield', 'Sucrose Content'])

# Convert 'Date' to datetime format and ensure other relevant columns are in numeric format
df['Date'] = pd.to_datetime(df['Date'])
df['NDVI'] = pd.to_numeric(df['NDVI'])
df['NDWI'] = pd.to_numeric(df['NDWI'])
df['CIG'] = pd.to_numeric(df['CIG'])
df['Sucrose Content'] = pd.to_numeric(df['Sucrose Content'])

# Prepare the data for regression
X = df[['Date', 'NDVI', 'NDWI', 'CIG']]
X = X.copy()  # To avoid SettingWithCopyWarning

# Convert 'Date' to a numeric value (e.g., days since the earliest date)
X['Date'] = (X['Date'] - X['Date'].min()) / np.timedelta64(1, 'D')

# Add constant for OLS regression
X = add_constant(X)
y = df['Sucrose Content']

# Perform the regression
model = OLS(y, X).fit()

# Print the summary of the regression
print(model.summary())

"""# **Fixed Effects Model**

A fixed effects model accounts for entity-specific heterogeneity by allowing for a unique intercept for each entity (e.g., Farm ID in your dataset). This approach effectively controls for all unobserved, time-invariant differences across entities. Here, we cannot directly include time-invariant variables (like Farm ID) as regressors, but we control for them through entity-specific intercepts.
"""

import statsmodels.api as sm
import pandas as pd

# Assuming 'df' is your DataFrame and it now includes 'rainfall' and 'humidity'
# Convert 'Date' to a numeric value (e.g., days since the earliest date)
df['Date'] = (df['Date'] - df['Date'].min()) / np.timedelta64(1, 'D')

# Create dummy variables for each Farm ID to include as fixed effects
fe = pd.get_dummies(df['Farm ID'], drop_first=True)

# Prepare the model data
X = df[['Date', 'NDVI', 'NDWI', 'CIG', 'rainfall', 'humidity']]
X = pd.concat([X, fe], axis=1)  # Add fixed effects
X = sm.add_constant(X)  # Add constant
y = df['Sucrose Content']

# Fit the fixed effects model
model_fe = sm.OLS(y, X).fit()

print(model_fe.summary())

"""# **Fixed Plus Random Effects Model (Mixed Effects Model)**

A mixed effects model includes both fixed effects (as in the fixed effects model) and random effects, where random effects allow for random variation across entities (e.g., different slopes for different farms). This model is particularly useful when you believe that some of the variation in your dependent variable is attributable to differences that vary across entities but are not directly observed.
"""

from statsmodels.regression.mixed_linear_model import MixedLM

# Prepare the data
X = df[['Date', 'NDVI', 'NDWI', 'CIG', 'rainfall', 'humidity']]
X = sm.add_constant(X)  # Add constant
y = df['Sucrose Content']
groups = df['Farm ID']  # Define groups for random effects

# Fit the mixed effects model
model_mixed = MixedLM(y, X, groups=groups).fit()

print(model_mixed.summary())

"""In both models, we added "rainfall" and "humidity" to the list of independent variables. The fixed effects model (model_fe) controls for unobserved, entity-specific heterogeneity by including a dummy variable for each Farm ID, effectively giving each farm its intercept. The mixed effects model (model_mixed) goes further by allowing both fixed effects (as global coefficients for your predictors) and random effects (allowing for random variation in intercepts or slopes across farms, defined by groups).

Please note that these examples assume that your DataFrame df now includes the columns for "rainfall" and "humidity". You'll need to adjust the code to match the exact structure of your data and ensure that all necessary libraries are installed. Mixed models, in particular, can be complex to interpret and require careful consideration of the model structure and assumptions.

## **Test the best of three models**

The Hausman test is a statistical test that is commonly used to decide between a fixed effects model and a random effects model in panel data analysis. The test evaluates whether the unique errors (ui) are correlated with the regressors, the null hypothesis being that the preferred model is random effects versus the alternative fixed effects.

However, applying the Hausman test directly to compare "Pooled OLS", "Fixed Effects", and "Fixed plus Random Effects" models is not straightforward, as the test is specifically designed to compare fixed and random effects models. To approach this systematically, you could first compare Fixed Effects vs. Random Effects, and then separately assess whether Pooled OLS or the preferred model from the first test is more appropriate based on theoretical considerations and model diagnostics rather than using the Hausman test for the latter comparison.

Here's how to perform the Hausman test in Python using statsmodels for comparing Fixed Effects and Fixed plus Random Effects models. Note that performing the Hausman test requires the estimation of both models first. We'll assume you have already fitted these models (model_fe for Fixed Effects and model_mixed for Mixed Effects) as described in previous responses.

Step 1: Fit Fixed Effects and Random Effects Models
For demonstration purposes, let's assume you have already fitted these models:

Fixed Effects Model: model_fe
Mixed Effects Model: model_mixed
Step 2: Perform the Hausman Test
The Hausman test is not directly implemented in statsmodels for these models, but you can perform it manually by comparing the coefficients and variance matrices of the fixed effects and mixed (random effects) models. Here's a way to manually calculate the Hausman statistic:
"""

import numpy as np
import scipy.stats

def hausman_test(fe_estimates, re_estimates, fe_var, re_var):
    """
    Compute the Hausman test for fixed effects vs random effects models.

    Parameters:
    - fe_estimates: Coefficients from the fixed effects model
    - re_estimates: Coefficients from the random effects model
    - fe_var: Covariance matrix of the fixed effects model estimates
    - re_var: Covariance matrix of the random effects model estimates

    Returns:
    - Hausman statistic and p-value
    """
    # Calculate the difference in coefficients
    diff = fe_estimates - re_estimates

    # Calculate the variance difference
    var_diff = fe_var - re_var

    # Hausman statistic
    hausman_stat = np.dot(np.dot(diff.T, np.linalg.inv(var_diff)), diff)

    # Degrees of freedom
    df = len(fe_estimates)

    # Compute p-value
    p_value = 1 - scipy.stats.chi2.cdf(hausman_stat, df)

    return hausman_stat, p_value

# Extract the estimates and their covariance matrices
fe_estimates = model_fe.params
re_estimates = model_mixed.params
fe_var = model_fe.cov_params()
# For mixed model, extract the covariance matrix for fixed effects only
re_var = model_mixed.cov_params().loc[fe_estimates.index, fe_estimates.index]

# Perform the Hausman test
hausman_stat, p_value = hausman_test(fe_estimates, re_estimates, fe_var, re_var)

print(f"Hausman Test Statistic: {hausman_stat}")
print(f"P-value: {p_value}")

"""This function calculates the Hausman statistic and its p-value. A significant p-value (typically <0.05) suggests rejecting the null hypothesis in favor of the fixed effects model, indicating that the unique errors are correlated with the regressors and that the fixed effects model is more appropriate.

Remember, this test is specifically for comparing Fixed Effects with Random Effects models. The choice between Pooled OLS and these models would typically depend on whether you believe your data has significant cross-sectional or time-series variation that needs to be accounted for with entity-specific intercepts (fixed effects) or if you can assume homogeneity across entities (Pooled OLS).

# **Measuring the prediction accuracy**

To evaluate the prediction accuracy of the Pooled OLS, Fixed Effects, and Mixed Effects models, we will use the Root Mean Squared Error (RMSE) as the metric for this example, given its common application in assessing the accuracy of continuous variable predictions in regression models. Note that to directly compare the Fixed Effects and Mixed Effects models' predictions with the actual values, we need to adjust our approach slightly for each model, particularly because the Fixed Effects model involves within-entity variations.

Step 1: Calculate Predictions
First, we need to generate predictions from each of the models. For the Pooled OLS and Fixed Effects models, this process is straightforward. For the Mixed Effects model, predictions are also direct but remember to account for both fixed and random effects.

Assuming you have already fitted models named model_pooled, model_fe, and model_mixed (note: the actual fitting of these models is not shown here), the following code snippets show how to calculate predictions:
"""

# Pooled OLS predictions
predictions_pooled = model_pooled.predict(X)  # Assuming X is your predictors matrix

# Fixed Effects predictions - we use the same predictors but must handle fixed effects
# Note: This may involve creating dummy variables for each entity and subtracting entity-specific intercepts if they were included
predictions_fe = model_fe.predict(X_fe)  # Assuming X_fe is adjusted for fixed effects

# Mixed Effects predictions
# Here, we can directly use the .predict() method as it accounts for both fixed and random effects
predictions_mixed = model_mixed.predict(X_mixed)  # Assuming X_mixed includes the necessary predictors

"""Step 2: Calculate RMSE
The RMSE can be calculated by comparing the predictions from each model to the actual observed values. Assuming y is your vector of actual observed values:
"""

from sklearn.metrics import mean_squared_error
from math import sqrt

# Calculate RMSE for each model
rmse_pooled = sqrt(mean_squared_error(y, predictions_pooled))
rmse_fe = sqrt(mean_squared_error(y, predictions_fe))
rmse_mixed = sqrt(mean_squared_error(y, predictions_mixed))

print(f"RMSE Pooled OLS: {rmse_pooled}")
print(f"RMSE Fixed Effects: {rmse_fe}")
print(f"RMSE Mixed Effects: {rmse_mixed}")

"""This will provide the RMSE for each model, allowing you to compare their prediction accuracy directly. Remember, a lower RMSE indicates better prediction accuracy. However, also consider other aspects such as model interpretability, the significance of coefficients, and theoretical considerations when choosing the best model for your analysis."""